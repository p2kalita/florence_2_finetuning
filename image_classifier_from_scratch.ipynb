{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6625f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e585605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88661001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638f4857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68353f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = 'C:\\\\projects\\\\vision_transformer\\\\dataset\\\\train'\n",
    "test_dir = 'C:\\\\projects\\\\vision_transformer\\\\dataset\\\\test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0626d",
   "metadata": {},
   "source": [
    "# Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828e061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19da77f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually created transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])           \n",
    "print(f\"Manually created transforms: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308f1c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2430dd54af0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x24328b57220>,\n",
       " ['Bad', 'Good'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the batch size\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd042d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgtklEQVR4nO29yY9tWXbX/917n/42caN7TebLzKqyXS5LJYuSLQZGDFwDhKmSB0gMXGKGJQRCyMwYAbIQMhJCBslmBK4B2BOQLIFt0Y34AyiZH9jVuCpdVfky33vxIuK2p9vNb7DP3vfceNHH7SJifaTIjHebc/Y5N+767r3W2msxY4wBQRAEQQDgmx4AQRAEsT2QKBAEQRAeEgWCIAjCQ6JAEARBeEgUCIIgCA+JAkEQBOEhUSAIgiA8JAoEQRCEh0SBIAiC8JAoEMQtYYzhn/yTf7LpYRDEUiFRIB4U3/zmN8EYW/h58uQJfvEXfxF/9Ed/tOnhEcTWE2x6AASxCn79138dn//852GMwatXr/DNb34Tf+2v/TX85//8n/H1r39908MjiK2FRIF4kPzSL/0Sfv7nf97/+2/9rb+Fp0+f4vd+7/dIFAjiEsh9RDwKBoMB0jRFEMznQf/iX/wL/MIv/AL29/eRpil+7ud+Dv/xP/7Hd95bliX+wT/4Bzg8PESv18Mv//Iv48c//vE6h08Qa4NEgXiQDIdDHB0d4c2bN/i///f/4u/8nb+DyWSCv/k3/6Z/zb/6V/8KX/nKV/Drv/7r+Gf/7J8hCAL8jb/xN/AHf/AHC8f61V/9Vfzmb/4m/spf+Sv4jd/4DYRhiK997WvrviSCWA+GIB4Qv/M7v2MAvPMTx7H55je/ufDa2Wy28O+qqsyXv/xl89WvftU/9q1vfcsAMH/37/7dhdd+4xvfMADMP/7H/3hl10IQm4BiCsSD5Ld+67fwxS9+EQDw6tUr/Pt//+/xq7/6q+j1evjrf/2vAwDSNPWvPzk5gVIKf/kv/2X83u/9nn/8D//wDwEAf//v//2F4//ar/0afvd3f3fVl0EQa4dEgXiQ/MW/+BcXAs2/8iu/gq985Sv4e3/v7+HrX/86oijCf/kv/wX/9J/+U3zrW99CWZb+tYwx//uf//mfg3OOn/iJn1g4/k//9E+v/iIIYgNQTIF4FHDO8Yu/+Iv49NNP8d3vfhf/63/9L/zyL/8ykiTBb//2b+MP//AP8d//+3/HN77xDRjqUEs8YmilQDwapJQAgMlkgv/0n/4TkiTBf/2v/xVxHPvX/M7v/M7Cez766CNorfFnf/ZnC6uDb3/72+sZNEGsGVopEI+Cuq7x3/7bf0MURfiZn/kZCCHAGINSyr/m448/xu///u8vvO+XfumXAAD/+l//64XHf/M3f3PVQyaIjUArBeJB8kd/9Ef40z/9UwDA69ev8bu/+7v47ne/i3/4D/8h+v0+vva1r+Ff/st/ib/6V/8qvvGNb+D169f4rd/6LfzkT/4k/viP/9gf5y/8hb+AX/mVX8Fv//ZvYzgc4hd+4RfwP//n/8T3vve9TV0aQawUEgXiQfKP/tE/8r8nSYIvfelL+Df/5t/gb//tvw0A+OpXv4p/+2//LX7jN34Dv/Zrv4bPf/7z+Of//J/j448/XhAFAPh3/+7f4fDwEP/hP/wH/P7v/z6++tWv4g/+4A/wwQcfrPWaCGIdMENRNYIgCKKBYgoEQRCEh0SBIAiC8JAoEARBEB4SBYIgCMJDokAQBEF4SBQIgiAID4kCQRAE4VnK5jXa6kAQBLE+2pV8l82dRaEtCO530giCIIjlwthcDIwxKxOGpZW5UEpBa0OrBoIgiBXgRMEVc1yVMCzNfeR+AEMrBYIgiBVgjAHnfLvdRw7OBTgHOF/dYAmCIB4r84n3PXAfGWOXNoAd5CpVjCAI4jGzak/MnVNS2wMkLSAIglg9qxQG2qdAEARBeEgUCIIgCA+JAkEQBOEhUSAIgiA8JAoEQRCEh0SBIAiC8JAoEARBEB4SBYIgCMKztDIXbYwxgDGoagnAIAxD/7hUCgwMYRj4ok5n3+tqKGltoLRBGAgIIa48p9YaMHAbq8EYW9hdLaWClBJRHIEBqOsanPOmRAejndgEQTx6ViIKaAx7WZYwxkAEgX1Ma5RlBcasKLTreDi01t7Aa61RVhI8TcD51Ysarex7GGdAIwhCCDAARmvUdY28KCGCAJwzVKX9PQgZGBcgSSAI4rGzElGQskZZFnj58hW0AT744AUEB5gxmM1yMM6RpYmfmZdFAa0NpNZQsoZWCiIIobRGWdYIBIcQ/Nxa4k5QGGM4OjpCnucIghBBFCGKE3S7GULBYZRCWRQYjydIkxicc0ynU0RxgpgJCMGpTgdBEI+elYiC1gZKSmitobRBXdeojYbRClVVgTGO6XTWlIAFZtMZlNbQhgFGgcEAjENpjVrWyIsCWmsIzqGNhpQKIhDgjMNoDS4EwjBEUZaYzXIEQY1QKigNcA6EQgBaIs9z5HmJ6SxHwDmqqgYPQkRU65sgCALAymIKGkpp9Hd2oLVBlU8xnswwGk+RpAkYgKM3rxFFEcJA4OjtCbQGojTDTjdFJ40hQgOtNOqyxKenp6hriSyJUFY1ToZj9Ho9RHEEXVXodLvYf3KIoihRliWKogAXOcR0irdHdkwhUyhqhbxUmE0mCAKBMI4gwngVt4AgCOJeshJR4NzO3POTMaqqQhhwFEUFKSXKsgSDDfoaU0FKgV6vB8Y5RBAhDLkPFIMBrAkEC65Q1TWUUggDG6SGAYIgAGNAVdUwWvsAt9AGBsyuRgDU9ikIzqC0hpEAuITSahW3gCAI4l6yGlEQHGEYoiwKzGYzBGEAJRW0Uqgqa/OtoZaQSuPwcL8JPHMwo8CgXWcGMGbjCVpzVGUFY4AoinwznyAUAGNWFJqspVpKaAMwLiAYA2ONKIBBCAajAaU1WOPiIgiCICwrEQXBBVgc4+BgF+NxiJefvkISJ8iyDsqqhIGBEAE45xCBQJJECEQAqazLCI2d1lqhzHMUZQkpawghILhAEITQWkEriUlRI0mBKMnAGYPgHAw284iBIQg4OOdQMDDawCjAMJttlMQRwmA1CVgEQRD3kdUEmo2BVgpKKZsiCmbdQ0KAc+6zhzhn4AyQtYTRgDYMRtmAdJ4XqKWEbI6jlG5SSXlzDA2jbdBZNbN9zjmECBBFkT0+Y9DaNK+1oqCNAWMcnLNWRhMFmgmCIIBVpaTWFfLpFG/evEVelOh0MgRBCDAbAwBg9xHAwGiFk9MhOBeIkxRQFYyWOBnOYAwQhAHq2rp5OONzUdAMGqYRDSsKYRRBCI4oTSFriaosURRV4yIysP9liKIAXHC7nwFNFyPSBYIgiBW5j0SAOEmxt7+HupaI4wgw1iC7rQBKawjGwDmgtHX32KBxCmY0RJjaAYYBVCMKaZbYoLMQjftIoZsXiOMYvV4HOothtIYBoJSCrCWkVHZ1AA0DBsMYOOwqJYojJEmCMBC0RYEgCAIrCzQLhHGMfr8PrTWSJIaSdkYfBLZcRS0lhGj8/Wq+Ac2WmwBEGIM1Kwut7C7nKInAm53KrhxGmpYIggBxksBN9xlj0Ep7t5N1VxmAMRgwaGUzjoIgRBAIBEJQiQuCIAgAzJwtPnRDrM/egDG8Y1hdZs/FO5GtALgRtN/e3qnc/v0s7pjvPGcAA1dG4+Jjo7V6IWEgCGJbcRNhZy9XVa9tJSsFN9CrititFAYwqmZEEARxI6h0NkEQBOEhUSAIgiA8JAoEQRCEh0SBIAiC8JAoEARBEB4SBYIgCMJDokAQBEF4VtRkhwoJEQRB3EdWVjfaGKCua78LjyAIgrgebgNwGNrK0OtkRSsFu1rgnDVbsmlnMUEQxG1olwdaBytaKdha1JxzX5eI6goRBEFcD2OMrx23blFY6bqEhIAgCOJ2bMp+rtxZRcJAEARxf6CUVIIgCMJDokAQBEF4SBQIgiAID4kCQRAE4SFRIAiCIDwkCgRBEISHRIEgCILwrKz20XlcVgPpqv0MV9ZPYgxYYo2l88bTHsPZ5y977jrPEwRBbANrFQUAvkCeMQZ1LSGVRJZmEOI6htLAGECbpoQGYLeCM2aXPI3h1ab9evujlIYQHEEQQGttjbTRUNpAGyAK7a2o6hphECAIzr81Wmtbyakp4UEQBPGQWLso1HUN09T0KMsKZV0jTRJcx5NV1xWkVCiqGlmSQAiByWSCIAiRZCl0XUNrharWMNBeJIwxKIoSaZqi08lQ17UVBq2gDaDBEHAOwKCsKnDGzhEFe6yqLME4RxzHC7N/rTWkUgiCAOICsZB1DcY5uBBgoBUDQRDbx9pF4fT4GFLWiOIEo8kM06LETq934czcobXG8Zs3ODkd4YcvX+OnfvJz6Hc7+N/f+mMMdvfwEz/1RUxOXiOfTfHqeAIla+i6RJKm0Nrg5aev8eFHH+CnvvhTOD0+QV2VgK4BEYOHCaLnVpROT4ZguzuI4/icMSh89ulnCKIIz957DsG49VpphaIocTocYW93gDRN332vUjg5OUYUxci6PQSCVhoEQWwfaxcFKWvIWkIEGlLWqMoCP/rRjxCFAeI4xizPUZUV0k6ncQ9JdHs7SJIEcZIiSWskcQDOBcAFBjt9pEmMqqpQ19K6pKqycStx1LWCgUGv20EYhqilhKxL1FUJKWuIEAgM8Omnn4FxDoDj7dFbHL99iyzLoI3BdDrD3t4uOp0MZVFgNstR1RXCMARnHMVsilme42Q0xuH+PjqdDqIoRhAIBEGA2XSCPM/x6rPPMNjbx/MXAXqddO110gmC2Axaa+R5jrIsMZvlyLIMURSi0+ls3eRw7aKglIJUEqHWUFJaUTg9huAc/X4Px2+PMR5PsXd4CM4YVF3g2XsfYLC3j16WIasl0jiEEAKcB9jdHYDzAFVVQUoJJSVUXQKMgwUhaqXBGdDv9xBFEaSUkHUNWVfIywqRtu6l4+MJhBDY3z/A8PQUk8kYBwf7UErj9ZsjMPYTiOIYZVEiz3O8PnqNOE7BhcDo+AjTvMDJaIzJeIxer4det484jpAkMY5ev8JoOMInL1/ivQ8kenuHyJIY4drvPkEQm8AYg8lkguFwiDdvjvD06RP0ej1kWUai0IYxDsEFQsEha4lXn71CUZaQSmI6mSGKQmRJhNlsBqWBYH8H0+kUp8MJnr/PkCQxwkDY4LOUqKoKeV7gzdFbZJ0udve7KKZjFGWBH5+O8f4HL9Dt92GMhtYKeW5V2wB2VaA1Pvnxj5tAtMarz17BGAOpFCbTKeLTIeqqRFmVmOQF8rwEZwxpmiKIYkRJDC4EiqJAwDimEwOplY15dHp4/733cHCwj06aQAhaJRDEY0Ephbdvj/G///e38D/+x//A17/+NXzxiz+Fg4ODTQ/tHTYqCsYYaKMhpYKU1vXDGEMYBvY5pVBLAEUBZYC6zqCUBm+U1QBQSkMbgEECYGCcwxgbyGaNz58B4JxBK4V8lkNJCa01lNZwDYG0VtDaIM9zhGGIIBAo8wIGgAgE6rrGbDZDLSWMXmx6YQzAOEMUx3YMjMFoDakkiqpCrx8jDEMIo5DE1q3EqBsdQTwaGGMQQvikF4BZF/gWslFRkLJGUeQYjkaAAeIoRZyGCCOBujSoqxpH46GNJSQpBjt9GDAM+l0wAEVRYDQaA7AGOYxjdDhHr5MhjEIAGiIIkXYC7B0cQiqFz15+gjjkNlUVzLp44gjj4SnqWqJUQNbJ0Ot2cCoVACDJUpR5jjd5jqooYRhDp9tDKBg4A8bDEzAuIJIMSZIgEAKsrqG0glIKaZYiSzPIgCNLm+e3bMlIEMTqEELg/fffw3j80zg+fosvfemLePHi/a2MK65XFIyBrCvMZjMMR1NMZlMURQGlDTiYndUzgIFBK9kYVY04SdHt9yGYQS0rTGczHB29QTiM8NnrN+j1+9g9OAR0jYoBUZzAMKDIZ2BN2uhoPEan08XewSG4rqwg1TMwxqCNXS0ABmAceV5AKwUDgAsBzjk6vR7iOMHJq88gtYZyqxWj7TG0Rp3nCIIAHAyymKEoK+RFieFwhKqswGSNKM1QS4mYAgoE8WhgjCGKIjx//gw///M/h8PDQyRJsulhnct6dzQDUEqiLEocD6eQqoZUEgCa/H0Ozq0RBnNGmiFJU3R7PQhus5HyooA5PQXjHMenQ0RJiqzbhSqngFGIk9RmI5UFwigCDDCb5ej2d7C7tw9djlGVBUbTym+CY4yBCw7BgiZDqUKWdayRFwLdbg+9fg/F6BRlVaM0DIwZwDAIzq2rq65sPAJ2T0Vd16ilwmw6g6olQg6UVQ0pVSNEBEE8BpwoHB4eotvtIsuyK9PwN8VaR8UYw97eAeIkA4+GiKIQYRiiLEoIIdDr9iACDi4YVK1RVRVG4wl29/fQ6XTAtYQQATQEoigEa9w/vV4fQgiwIARPGD73+S9AKQkla4RhBAOD4WiM3b09HBzswtQZlFLo7x7aWIKSmE4HAOPodLsoCpsW2+/vIAgEOGdIswxhFOP9Dz6A0hoKDEZJaCUxmeaAMeCCIUk7EEGIen8ApQ2ktvsYGIAgDJGmGQJuV0UEQTwuwtBlTm6f28ixdqmKkwRgDFLDi0KV1uCcI8sycM7AOINRBrWUCEKbyxsnMaAEwDgGhtlgLWPgnCFJUoRBAIMIJgjAwwRaKxglIYLABotFgE63gygMYDiD0QpchNBaQSkJEURgnCHLOkjiGHWzUhBCgLH5h5llHWijoZ0oaAUhIoABgWAIohicC6iQQxvAgEOpGsYYCBEgiiK7+iBVIIhHhws4bzPMXFlp7nK0trWFbDzA+dndY8w/xhi7uqjdI4OCzQRBnEe7RhwAv7Kwj6F5jK3EhqzdfUQQBEFsL9vr2CIIgiDWDokCQRAE4SFRIAiCIDwkCgRBEISHRIEgCILwkCgQBEEQHhIFgiAIwrPyfQpuExttXCMIgrgemzSXKxUFJwTG0MY1giCI62J85WYAa+69shJRcAKglNqo4hEEQdxnhOBrL565spXC2TpIBEEQxPWYe1ZWU9/oMlbqPtrWeuEEQRDE+azUfUQQBEEsh3V5XCgllSAIgvCQKBAEQRAeEgWCIAjCszRRMGazGy4IgiAeMuuyr0sKNJtGFAw4p93LBEEQy8a14my3Pl4FdxYFxgBjGIzRC/1DCYIgiOXhbKsQq/X6L2WlYFcH9ncSBYIgiOXjFgeMrXZD2xJWCswXvbPQaoEgCGL5MO862mpRcHBOiUwEQRD3naWIAu1gJgiCeBjQ9J4gCILwkCgQBEEQHhIFgiAIwkOiQBAEQXhIFAiCIAgPiQJBEAThIVEgCIIgPCQKBEEQhIdEgSAIgvAsZUczlcomCIJYH1td+6gtCCQOBEEQq8UVIXW/L5ulFcSTUkJrs3JhOHt8qrtEEMR9YW7MAeBmtstVSA2CYLtXCoC7UIZ12Gfb1McJw2pLyBIEQSybRXt5U/vFfPe1VbG0lYIQfG0GWmsNYPV1xQmCIJaJMQZaa3B+c3vpWh6746zK9i0hpoB3lGuVhvq8m0HCQBDEfcCtEtpcZL+MMZhMptBaQwiBMAwRhsHKm5gtbaVAEARB3JyL4rDGGEynU0gpEUURsixDGAbNc6tzIZEoEARBbIC2GLjVgnusLEvkeYEf/vATFEWBIAjw7NkTPHlyiDiOV9rpciWicJ7ytdOozuPsTWk/RhAE8ZCwZs6gqqomNsoBWNe4EAJaG+82CoKgEYHLbeiyWNlK4TLjftVzxhjq+UwQxAPGQCmF4XDs00wBa/e63S4A2/d+b28HdS1hjEGaJmuZKC9VFIwxUEqjKArMZjkAQAiBnZ0eOOfgnGE0HKKuJWptkCYJkiRBFIaA0VBKoSxL1LVEt9eDEFYYlFIwxkAE5O0iCOJ+Y4xdIVRVjdls2qwaGHq9biMOdrLMOYeUCnUt/ST53okCAGitUJYVJpMJAIYwDNHpZAhDBhggn+Uoygq1tlF4zgUEt0unuqqQ5wWqqkKSpgACMAbIWkJphUQIcikRBHHvkVKhqmpUVe03/Xa7HW/fnCgoZSfL7cdXzUpWClorv5dAaw6tNZSUAGeopIJUGqFgmE4mGI8nSJMYRitMJ1NUVW1vhGEQgkOrGnleQkmNDz/3IcIwAGcUbyAI4v5iYwccQRBcWrLCGO1d6usqI7QifwzzAZOg5fKxsQIBITSCgKEualSVBIOBURplWTWuImA6ndoNHtAoyxpan19n6aoANkGsGpqgEDfFxRDiOAYwF4mzMGbd7uv8E1u6KLj6HJwLGy+IQm+4tTEIwhCMM4ScIc9LVGUBrewSqqwkAs4gODA8HYJzhiAQUJqBcdHawTxXTbciIYh14/4eV7m7lHh4MMYQxxHCMICUqom38nP/joKAQ2vhY7LrYEUrBZtOJaUEYwxpandaMDBoraG1AQRHEAhEoUBR2ZWACAKEoUAgOHRegnGGOI5RKw3GxFwxjQ0+a23WqqAEsYid3bkvNQkDcRM45+h0On4iPa+JNE/Pt54VZ+fuqSi42ZO7SGN0a4bvREFBa/slEoGAyksYAwRB6L9gnHMwzsA5gzAcYDZPF42nyNUQoe8hsTnmy/qLZnoEcRGMMYThYsVTa9dMy47OVwjr+ttaqijYiwyRpklj/Oc1O3gjEnVhs4+mPEDAgbBVBpazZiefUgjiBACQ5yWEsCsIqRQYZxAMfmZGX0Jik9hNRoYEgVgazm1uTAi3XYtz61k5KyKrYCUrhSAIkCSJrwZoA8YAGEOn20EYxzCMI+AMnDEwbos8hWEAJW3mUhBFAACjFYQQECKwS/XGf9S+Mff5y3gbY/JYDNC2X6cd36ZHQdw/DIx5144Bc1vGmBWBbrfbxB3Yystb+DGYO6buzGdJi8a5HQB2s3qbsqreScFSSvl/W5cT5hdvDOCC10L4x7TWK+0+RBBX4f5WASzEFgjiIpwHpe1mvwntvzkbY93izmtnuejLcfZxt0mjzUIFwFalWS8aSx4rMWfbZ+fXYZ3XcM9vFbEh6lp6cTgf07Ql0L6Pggs2R1HY2EyxkrGtRBQuutCLlPE2arlJbmp0NmFob3vOy95zXwRjE2Nc5+Yi4n7TTsJx/WjOo+1Zcd89xuDLZ68KKiZ0C25qdDZhpFZxzlVdx30RG4K4K+7v3DbLEZdOJIwxKAoNYzTc3qx297VVQaKwxTwWY7ntq67rQIsE4ro4l/lVgmD3KTAAfK0tiJciCme/qKtUsnUt0697jvPrlZzfT+Ku57oLV51jGw1tm/ny+fz7ve3jJwignV10+d+rs3OiSa5Z59/3EkTBGhsp1RWGZ/6c1gZSSgjBIYRAXcsmlfV6gRMXiL7NjbrKgLSfdz69qqohxDyDinOOMAwvPY9rnhEE725OOa+HRDsroV0+dxm447uSvUopSCmRZVmrgcfF790Gg3vZGK4zvm25DoLYdpbmPpoHQ84+3i5e5zb7KOR5jji2exGKomjKYaQ+mHLxedAc6/IvuDOy7fIDzji2H2s/d3YmqrVGXUtMp1OfciilRBiG6PV6FxpTZ3zt/orFct/njVtKibKsIGUNxhiyLIMQwh+/Pe6bpjy6z0VKBaXstdgSIdpuKrzimMsUpk0a5YfgoiKIdbAEUbCbMOwy5/xZsJQKbqVgjEZVVXj79giDwQC9Xg+vX79GEAR48eLFlSl+193yXVUVJpMpOh07Gw6CAGVZYjqdotPpIAxDvzS76HhlWWI0GuEHP/jYn3c4HGF3d4Avf/nLiKJooQps+5rH47E/RxAEC+c6y3A4xGefvcJwOIQQAh9++CH6/R46nY6vITWZTJBlGbIsu/wGncE1LppOp8jzAp9++ik450jTxK9izruGZXOfjOx9GitBLJulWAP3HbJ9Ri96fm7M3U9ZlgCA2WwGIQRGo1FjQDnqugYAP9vmnHsDWZZVs3GDI8vSxsVTeSNXVTXyPMdkMoExe0jTFEII5HmOt2/foq5rxHGMTqeDuq4hpfSzQ1ff3LmOtNYtdw5weHiILMtQliWqqoJtFMRaLqB5O1FjDKbTKVzdJyFc0wzZBJsEsiz1Ocv9fh8AcHR05Dsz2dVKhdPTIXZ2+qjr+p2NgMYYJEnSCLD096EsS0gpG1GZoqqqZru8FSmlFOq6hlLq1qsQ95kSBPEwWNIUkS34+M+6YM5zn3DObZ2jvMBsNgPnHCcnJ0iSGGEYYjabAQDCMEQcxwiCAHVdoyhKDIdDcG7jEQcHB6iqCuPxGGmagnOOyWSC2WyG6XSKKIoAWJfMbDbD0dER6rpGlmXgXCDPZ5jNZt6QJ0nqmwRFUQStNZIktpVdATx79hScCxRF6QXFNtq2guW6JO3t7UIpjfF4AiklAIMwDFFVNcqy9HEJzg+9KO3v70Mphe9+93soywpVVUFrt7I6RlVVqOvad2Ry8QHAYDAYwBiDPM+RJAmEEDg9PfXVZGezGaSUfpUUhlFzjHqhs9NNXCfbKgbk/iGI27ORlFRjDOq6xnQ6Q57nyPMcjDEopfzKQGsFIQKkaeJjAJPJFEVRYDQa+dcVRdG4pGq8ffsWSqkmkGp96Forv5JxrqzxeOKN+mg0xng8QhRFEEIgimIfG+l2e0iSGB9++CHevn2L0WiEH//4E8RxhMFggNPTE0yns4XSHU4EnRGezWZ+ZRRFIZTSqOsaURQhy1I8eXKIeRPvoV+duGDw6ekpqsrGG8bjMeq69gI0m+VeFCYTuyJxsQfrwhoBsKsfW6gwxQcfvIDWGkVRIs9nKAorIgDeiX+0P69NG9nrjmEbxkoQ95m1i4ILALtZdbtGknvOuYI4t5s1pLTulbIsUdcVjNHQmjXG1za+VkpCNsX0rFEw3kUzb3rNvWtFa408z1FVJep6PtuvqtqvQvp9W/U1yzKMRiMwxlorA4OyrJDnM7ga6M7FBdjguZTSz+znuxhNy+jbmk9CCMRx5FcCNqhtDZu9ZmmbDSmFoiga4dML988F650rCACU0v5agyBAHMfIsgx1XTeur8Xt8xexDUb2vq9eCOK+sHZRqKoKRWFdRkEQYjDYwWg0Bucc/X4fk8kEZVliMBg0bo4ARVE0QiEQhl3s7u4CsJlMx8fHjWgoJEmKJEnQ7XYbf3mFbreDLMvAGEOSJBgMBo2B1U0MI8Tu7i6iKERZVnj58lNkWYZut+PjB87dEkURkiT1QWPr/sqxv3/QuLmiJrtKN9dnZ+hOiPI894+VZenTRHs9G1T+f//vT5DnOXq9LpIkQRCIZmVgMBjsYzqdYjabQWuDKArx5MmBF5zxeIKqqjCbzRBFEcIwxOHhgc+E2t0doNvtIo7j5rxAmtogvHWl3TyecJ7LaRtn9Ms619nrJIiHyNpFwRlFIQI/e7aYZjt38y9j4DKW3AqivT/Bfjl14/KJkKYJ2p2Lztvo5N7rjHrbXeVmzEkSYzDYwe7uYGGPAeescWelfpXBOUccJ61zMTBmzpxzPg6llHfjRFEEzjnyPPdG3GUWSWn3RgDw+yEYY/53FzjnfF5M0MUo+v0+4jhu4hWicZ8t3g/3u82Mul1RrYvSbO8yo1+VUCzrmGevk4SBeIhsRBTcT3tTldZYKIdtXUR6wd1yNtvFGbYsS7G7u4c8dz52x7uzOsbQGMvFPQtOUJIkwc5OH/v7++BcLJxTCI4kiVFVlReFJJnHPOxx3g22u+O79qTOhw/M3T5hGPoeFNPptAkyq6apBvf3TgiB/f29M5lN8C4vK5ApgiBAnufenba4E5z517tr3AZf/KbPTxDEhgLNjNnG1aPRCJPJBErZFUJZlt7dcnT0FkIIpGmK09NTlGWBvb19BIHwKaLGAJ1OB1JKfPrpp0jTFGEYeEMnZe0NZxzHjcAYFEXhN285l4ubdcdxjPF4grKs0Ol00Ol08OzZUx80/v73fwBjdJO9YwXI7QwOgsAb4fYMPwjm2UnT6dQH2oMgwPPnz3B6OsTLl5/i9NQGmoMgQFGUkLJGGAYQwhp4W1XRusyCIEAURd6QZlmGqqp8sNqJlbtXLvB9cHAAuwIzODk58SuyJEmu3AOxDcKxCh7qdRHEbdiYKDjDmaZzQ2SM8emneT4DY9zPfIWwXYiiKGzSTJsLCAI/c0/TxLumpKy9a6Suaz9rjqLI5/iHYehXIUkSIwhCP4t3gWNnLIIgRBwn3shmWeYFwG6QCxdWP+29DZwLKCW9y8gFq8PQXotS1g1m9xpoRFGMurYpqXEc+w1wThSscNq2p258caxRlmETW0ma+2hXRDYgrbzbbB7wVn7FdJ4r5KyxvK3h3PZS49uacUUQm2BjVVK11jg8PES324MQHABrCkDZTVRu85o1qKoxltG5sQLrg6+atNK5O0hrjZOTE5RliR/96EfodDo4PDxAVdW+XIULYn/wwYsmuBv6fQrtXc+7uwP0et0mhRQ+3tDeEby4P6Nd2kM3mUU2G6muazx79gzdbgf9ft+LlHORuX0IdgxuVTQ32hfVX7Ib3WpwLvwKxb3eHdtlTtmMqwBh6NxIq+sYdtMgtPv8NmGULyu8RxCPgbWLgvNlZ1mGJEm8IXe4L+TcSDmjaxZm7u8ek7+TZ+92+rogsAvo5rlNF3UrDpsSGvu9Cq7OeXuXrzv2PL11bjjOM6icz3d6u9WI2z3s4gpuBeTO5VYa7RjAecd2rzn7nO+HfU6gvT1uF3twtY/cauQsZ+/1XQ31Td67KaNMYkA8dtYuCs4opWnaZMkE5xr7y2oFAe+mB15Ug8itLoyBr1Xk3CVxHL9jJM8a0/bM8azhvg5uhu/2MLjZehxH/vx3maVfdyzt+ykE964sIbgXh6sgg0kQDx9m7phX52r9uHRP4N0yF7bMwyJuk9V571kWbrbtAsyu0XVd1zDGGkc3/igKVzKW9k7n8Xjsj++C08s+33VwNZrcaduriFVxn3z0N9lrMYf51SpB3BVnt9xm1fbfmlvZX1SB4K5sZKUA4FqVOW9rSNqze+eKEWK+qmhnBtmbv7qORm0XU5qm/jH3gZ7V5HUYT+tmuvn77jK2+yIIwP0aK0Esm7WKwk2/bFe9/iIjdfaxy/5txeBGw7o17ayp88Zy3r+3iW0eG0EQy+Fe92i+jZFalZvqvDjEqs972RgIgiBuw+ryEB8o54VgriMAqy6JQIJAEMQy2GpRWKYhvemxFstCzDm7Ijjv2Oc9fhOhuO11b3stnm0fH3A/xkgQq2SrReGus9/LjPJFhrz9+qvOf1G20nWzmC56/qLHrzJYN8+YuRs3PdYyVjMXifVNj3ERtOIiHjtbIQqrmp1d9gW/rc9/k7P4ZRisZQrHVUK7bK6z2/g6Y7jLfaSVBPHQ2Uig2e1duE55hdsEUO8adD3r/rmL8bts1XCZe+oq15Ir43HRDua7cJPjtO/1Rde4znEte4/JfcoOI4hlsFZRaBs0V9vooh3Ejk1kGDkXhdt059pbAvOdwe1Zq+t+ZmsWqXf2SbSN5tkaRu3Oc2c7tLkft+v5bOE61w/ClQ7ZBLd1kd0H7vPYCeK2bKQdZ1GUePnyU+zt7WIwGNxZBJaNlBJlWeKTTz6BUhpZlmI6naIsKzx58gRC2EqnrivbaDSGELaG0NHRW+R5jn6/56ub2kJ7ttT1vOWoLYx3enriDf/h4eFC1dc8L3wp7d3dAWYz29O6vaPRNdTp9Xpbce8IgrjfbMR95Gr4GzNfNZydDbtCcWdn0u79bTdLu3AdgGaGbzeltWsPtY/nzucea++wbhtuKWsYoyGlLWRn+xoYVFUJpRSiKMJwOPTNflzVVdfHwRig1+shSWydI6VsJdOiKPz/TdNRzrXwtL2jK1/N1ZbZnrclddcahqEf/7w3Nfz1reJzI+EhiIfN2kXBCUIQBL45fVEUAOyO37qWUEohjqPG+FbesM8b5WhvGJ1Bd30Q3ErE1WJyjXDCMIRSyruEGLOVSqWUkFIiTdMF0THGrRgq33BnvmIofS/pOI7x+vUbBEHge0MrpXzP5KIomhLhHQghUNcSeZ5jOp02/ZeVr780Go3BGMN0OoHrGufGP78Pxhfvs+U65mXClx1bOAsJAkE8fDbWZAdgqKoK4/EY3/3u92CMxu7ubjMbrpteyAqzWe77GtimOPP+xYBt27m3t4cPP/ywEYQCf/qn3/atMpWyIhCGEWazGSaTCWzntBBPnz5FWZYoigLvv/+i6XiWNz2UYzx5cog8L3B6eoooihBFMcqyhJRWPJyhdnGAuq7R7XYQhiFmM7uimM1mqKoSZelute1F7Wb385VLu/mNge317EpwMHDOkCS23Hinky3EELQ2ODo6QhRF6Pf73rVEEARxUzbkPkITyFUASt9CEgCKovRuFTfrtzNj259YKdX0L9beIMdx4mfLUkqcnp6Ac+G7owHWfTSdTjEajeBKaidJgqIoUBQl9vb2EUURiqLwPR663S6EEJjNZq2AshU0twpxxfQA+G5uSZI0TWysYZbSrlDmRl/7Rj6uFWk7YG07rAEAa1ZFDIAtt+1WLMBclIwxKMsSgF0p3VQQHrJb6CFfG0GsgrWLgvWVl6jrys/+33vvPeT5DMPhEJzbx2x/4hBPnjyB7SkMbyD7/V7jfrFunyxLW+WgOfb3931cwnZS0zg6OkIcR3jx4n0/s2/3alZKgrEIT58+9e4Z1wiHc45PPnmJ4+Nj9Ho9f+y6ljCm9l3YAIAxDs7nGUFxHENKiaKwRtv2erZB5LquG4GbZxpxzlEUpW/MY9uKBhgMdhGGEbrdDtI0hZSy6eZmG/dYUQuv3RuhzUM2mg/52ghiFWykdLZzH83LaIum7aYGYzYtMwhsE5pOJ/OBV+tTF4gi0QSX2UIaJ2B7BAgRQCnZ9CY2rTaXwjfSUUqhLEs/0w+CwPdMbtcpdw2B0jRt3FHztpYuftBuE2qN+zzw62IZbmVjBaJAWZZ+ZWFbkApUlY0xuFWGEybOXWxkMdXV3U/bmjNYEKc2y5wtr3vmfZ1ig3cZF60kCGKRDYgCb2bTvAkGz/Pz25lBzhD3el3ked4YcNct7eyGrXlfBNfsRCk0M/HaN/Sxs/cAzgbYlQVrXFAxkiTxouEQQqDT6WAw2IHWCsfHNoU0DEOUZQWl9MImPGO0FwF33Nls5h+rqsqnljIG9Hpdf60/+MGfI89zAEC320GSJBgMBtBa4/j4rXexte6mF0cnZuff8+UZvXUb0OtuHrvtuEgQCGKRtYuCEBxBMO8apJR17ZRl5fP53Q8AlGXpxcKmaKqmOT27cGYMzDeGSSm9KJytheSymdxKQSmF169fN377DJxzlGWJV69e4eTkBJPJpLkGu/+g1+vCGPiObkKIJj5ixxsEAeI49n2Z61oiCELs7tr9GUpJnJyc+mC6CyJ3Oh1Mp1MMh8PmWucuqfPuJwAcHb1t9it0Lw003+dVw7K4r+MmiHWwMffR/Etp006VUn4DluuF7Ayb889boTDQWiGKUi8eQvCFHcDOwNuNX7Ix+gJRFHohccFclwHkDK5LM3UCopTyewqU0kiSGEIEiKIQnIvG2Nd+jGVpA+WuH3T7WM6d1Ol0wDlHXVc4OTn1opVlGeI4xs5OH0pJTKeTJtZhBcOuchZdKZy34yMKWZaeKx7XqRt0m8/yPhrY9h6X+zZ2glg1G9u8prWCMaJxFdl+xR999BGmU5v/3+v1IIQTBuZn0c4vn6ap37wFwLtdAODw8NCndo7HY2htdyXPc/sdDNPpFLPZzM+uO52OjxEAdn/D3t6eT0fd3d1FENhxR1EEzoU/dxAEGA6HqKrK92COotBvTrMb8uYBcyll4wqzwtfv972YRFHkRTIMQ+zs7Pjdy21BtZlc8JlMbgf0Wa7jl78NqzaqyzbcV9VqIojHzgYDzXO63S7CMESaphCCo67rJsDKffxASuk3tLmMIbcDur2D2BnXdqDYGN0Y8LM1gmz+fxzHiOPYxx3ahtcFmoUQkHI+E3fuovas3J1bKYkwjN5p5N6eoQLWZbW3t+c32tnVgD2muyfumtwY2hvslNLNSokhDAOftnuZsdvm2fF1g8p34b6ubghiXWxIFDja+f47O31EUYw0TXwOfpvzHgOcYVTNJriJn70fHBw0O6Y5Op3OpeNJ02Th33EcL/xbCIFut3vpMdoz8ySJL3nlu2RZdu7jZ8dxHi6Y7VYWbiV0WUrqNhvD6waVl30egiDmbEQU4jjC8+fP/GNamxvn1juEEOj1eojjxM+4LwtAA8tPa9wEzq0lBPdB8/ZK6DbX+BDui+M210kQxJpFwX0RXQkK5wpq7wq+zfHejRVc733XfXwbceLqOM9NddH7bvrcMu7Lug3xbe8BQKJBPG421nmtnWVkN169m0J50y5Xq+yKdbYPwjLfu+xx32asq7531zGyq7wPN3ndfDMkQTw+NlgQ73rcZNa2ytld+9g3Pc9V773LKukst21StA0z41UElTdxXoK4z2xFj+aLWHZe/WNgG+/XOsa07Jn9Nt5HglgHWy0KxO15bO4PMuIEsRxIFDbMVcb7tsb9rJF0u70JC90PgjgfEoUNc9UMd5k7j6+TdfNQuOpayDVJEOdDokB4Nm0kbyJK1zH6BEHcnLVmH132RV4sAXEx1/2yP6RZ7yq57n1fF8sUhvO4KN30pveBRId4qGwkJVVKiel06msOtb9gUkpIKeEa17uua27X7lW0UzKNMb45T7uO0NnXu+MvtsvUTd0k43+3lU1rX/fIncc10HHVVt15zhqa9r8vOq/rvrbYPGjRCLnXuWY/Z8uCz5sKcV8KxB2zPa6r7uPZ39tjbO8aP+/62mM8e7/Ou6az53bva1exPfv5nfeZLXbBu975zsMdr/15V1UFAFeWTiGI+8zaRcF9gYui8O0q20gpUdf1O+9zdY2u64N3xnA6nTbF9OIzr7HF8NyX34mTG1tVVUiSFIAtS+0a8EwmEzBmC9TZjnHct8UsihJpal9nz8cAmEbksNBGs20snRAWRQFjjC+MJ0QAY1yPaQF36Xme+3ad3W7X95oG7M5m26q09iXEy7JElmUIQ9vr2vWZcK1G67r2x3O1k9qiYNuOtnta1Oh2u76+lJTS12FyuBLiVrgCJEnsDba7H+4zOCtUbjyMMd+vO45jXyrdvdbVnLItXivUtUS32/HC4cqv28+MIcsyP1YATVc+6Y/ten+7/hfGGOR50RRrTJqWqubCelUE8RBYuyi42ddwOEQQBO/MutoVTx22kQ2/0ZfRGdkf/ejHmM1mSNMEdS1RlpWvJhpFkW+N+eLF+wjDEFVV49WrVzg+PsZ7770HpRTevHmDw8ND9Ho9fPzxxwjDEE+fPm1qLkU4OjrCaDTGq1ev8Pz5MwwGAxwePvGiMxqNAQAffvgBptMpXr9+A8D4aqiTyQSnp0O8evUKxhg8ffoEWZY1IqLOCKLBy5efNka3wk/+5E/i6dOn+OSTl2DMzmI//vhjHB+foNPpIM8LHB8f43Of+xx2dwdIkgR5nmM4HOJzn/sIaZri7du3KIoCs1mOw8MDRFHUWh0Aw+GwKfPNMBqNMJmM8YUvfAFZliFJEkynUy9KVugCvH79GsPhEMYYpGmKg4MD9Ho9RFGI0WgE1ykvSWLfBtV95sfHxyhL28P79PQUb98e4eDgAFmW+V4UnHPfL/v4+ASnp6eYTCb46KOPfA+LXq+HJInx7W9/G2EY4gtf+AKGwxHyPPcz/+FwiBcvXqDX6+L16zcoy9L/vXDOMRyOsLs7wPPnz/24B4PBtVdcBHHf2Ij7CIBvGKOUwnA4BGO2BedwOMRoNEKn04HWGnmeNxVAI5ycnACAdwe5Wa4zYEmSNjNJ+Nmem7X3ej3kuZ2JO8PiVgD2XAZ1bWewg8EAnU4GIezt+eCDF2CMQymFFy/eb3oY2JWIXdlIRFGEFy9eoNPJEIZhM+O0M1HXb8EYg7quMZ1Off8GIXLfr3l/f9+Ln5QKJyenZ5oNzWtHdToZdnaeo9/vg3O+0NMhTTPs7bnezSGiKMTBwT52dvrg3B5rNpv55kGnp0O/cnGrtPb1uZ4UrmVpHMe+Pao1rnaVVJYFOBfodjt+BWM/G4OyLJreEhplWTWtUiv0+z3fcc9NGIqi9CudNE3x/Pl7SNMEjDGMx5OmPIotaQ7YlcXOTr8pQ86a+ymbRkjMr2gmE9vNbjqdotPpeLeaW3kNh6e+RauUCpzrpr9F2HxWHEpplGXp+14QxENjQ3/VzBs6JwqAnZWOxxMMhyOEYQgpJYbDIbKsgyTRTWMea6zSNGs6nRXefz0YuJLT8y5jUWQL5WVZB8ZYA+JWHE5wsixDVVWQUvpS2UEgvAHq9boYjcYoigJPnz6FlBLHx8ewDX+Ud7v0+317dcz1atao67m7yL2+LEsvHGXJm65uCjs7OwjDEEJwPyNP08wLoKuIauMrCQ4PD5EkCQBrCAGgqhLfjKeua8Rx3PSYHqDb7XphCoLAG/bJZOLbgUqpmmuwxyyKAnk+gzHWNeVWB6PRCFprdDod7/KbTKYIAusqStO0WXFYd9zJyYn3z9d1jbIsMZ1OG8NthdC5uuq69i6mJEmQphkYs6J9cnLqu/A54ZdSYmdnB7u7A7x+/cYLTlVVC61dZ7MZptMJJpOJj8W4lqt2/BMIESDLMrgmTVEUIYrCJibCwZjt3+HqdhHEQ2NDf9XGf+mkVJhMpijLEsfHJ5hOZyhL64rQWjcz2rLp69wOIh771YWbYXY6HYRhgJOTE6RpisFggGfPnsE1tamq0rtB2l9qKaX3o9d1jdFoiLIs0el0wBhDnudN4Jah08ka9xf3Pn7XsvPk5AS7u4PGqLQ7zM1bexqjwZpOa/bYhQ+Qvn79BpxzPH16iLqum7ae0vdhBtD4ue3qgjGGp0+foNvt+vG7+ITzmzsxsfePNysCeAMNwK+msixrBA2+qdB0OoNS2q860tSurEajke9Fzdi8wKExBmVZIY5tXERreOPrPm/7HjsxsKJZ+0lAnhfN7F40s/ch3rw5wsHBvo9ftJMJ3HHc+N0Y2j/uMx4Oh42ht/El10DJrZhsX4oYWZb6VdCzZ898rwp3316/foPd3YGPU5EbiXhIrF0UlFI+uOuCly746gxZFMV+9u/8w1obCGHfP29mzxFFkQ+cOmPRbjbj3u9cQ+7cLpB49t/O122MXXVobTAcDr2IuJm6C1afzzwrqSwrn7UipfJGkHPhO8edl1Xjftz4zuKMnQuMuuCsWwG4ntdWfBbf697jxuWE4mwhPScy7r7aH94a62L6plv9SVm3xjdvhCRE0FyLFVgrGtqLsvtbsMFnAa2lP7Y7f/szaz/uAtZulVCWpc9aqqq6+ftSXsjaGU5u5eJcXfNEBO6D0G2xuaqREUHcZ9YuCu5LW9eVn323v8SDgZ1pn55aN0G/38d0Om2ygRKUZYnRaORdJG7V4Yy5m92dTQmta9nMVKU3dO1VQzvlMUkSRFGEw8ND5HmOTz75xAemL8pxd+6XKIr8zL6qKoxGI5yeDht3TIHpdIrT0yE6Helnn3EcIYoidDopOBde1IIgbALNcuF8QgiEYYBOJ/OGcDgcQinl3XFSShwcHPjsKIc1wtZIK6X8487NNj8HhzF2BeZcLdYAX/zZWoFWXpQAt4oqcXo6RFXViKIISRJ7gytljbquFjKJ0jRp/PcKYRghimIkSbyQfqrU/LNzY9JaYzweYzweoyxLDAYDCCEwHA4XsrPs/XNZVjZuZV1ZLk5gA/qMcd/G1V6LnagcHh5eKxOOIO4jaxcF5+MNgqBxF+TeR+z8yOflyAP2C50kCQ4O9n2T++FwhCAQSJIepKz9l/tsGYMgsC6GbrfrA5RC8MYd0sV0OvUuAzs7FAiCEHGscXCwj+l05n3ndrVjFvYDBEHg3ThWFOys0652Im+cGbPtRV1AXErpg5m2r7Pw2ThxHCPP85Y7yLrBnADt7OyAc2s8XVxEa43d3d1G1A5QVRXevj0GMO9KlySxz3pq+96zTLZWQ9z/uGC3XZFI73py/ny32omiCHVdYzbL/Uze7Tmxrh3hDbtzR43HdRPUtS1aXZzA7WuI4whJkiJJUkhZ+xm7XTkKcC6a2E2Nqqr934l1+QQIwxAvXrzAdDr14uBSXF06sDufdV0J70pygf3235DWIWUeEQ+atYuCy793rgM3i3WzdRugnYuDndXPXSxulryzs4MoinzgMYoiH6h0huisMAjB38nDt+8NMZvN24IGgTXMLhaRpqkPRrq9BC7+4I7vxuBiCDZYan93qweb2cO8KLlZ7/y+CG8M3bhsPMMG2bW2M3Unoi5jR2uNJEm8ke50Ouj1uuj1ephOp83Vz++DnaVHKAoBKbl3J7lAu7tv7fO4z8u5Wtzs2QkRY0AYBgt+fACNC4chTTP/WboxuJXS3D0F7/JxhtfdB3t+5V1u9hhzF5zNgMrhkhjc3xljHP1+3+9DqKoaWrsMK3vdttFT5Fc47j6071l7U6FzbRLEQ2RDTXbQzEKtj3g4HEHKGr1eFycnpyjLEvv7+6iqCq9fv0a/30eWZciyDEWR482bo2bzW+Azl4ZDgWfPnkIIgZcvX6Lb7WFvbw9BYGd+P/zhD3F6OsTp6Slev37thcHm0gMHBwcIwwBVVeKTT36M4+MTHBwcQGuD0WiIXq+HLMvwf/7P/4cgCDAYDHB8fIw8z/Hee88xHJ7iO9/5DnZ3BwhDu3chTVPs7u5ib28PcRyhru1Gqffffw+zWb6w21oIju9973uo6xrPnz9Dmqbo9/teOI+Pj1vBVY7xeIwf//jHeP78Ofb2dvH8+TPkeY63b4/9yqMtpJzbWfgnn3zS7Is4xec//wV0Ohnevj3GbDbDaDTCwcGBN6o7OztI09RnZpVl6fc0fO5zn4OUEt/+9newt7fnXxsEVjjbfvckSbC3t4vhcNjMxjnCMFpY3TgBmrvgBDgv8ebNG3z729/BBx+8QJIkPkvJ3rfAi8cPf/hDzGZTPHnyBDs7/SbQzTEaDfHxx3+OKArx0UcfoqrqRtxzFEWJ4XCEDz/8EP1+H0dHR/4+7O/vLwT4gfkO+Y8//nPs7e3i6dOntGogHhxrF4V2aQjOBeI4wu7uoNmd20EUxaiqCoPBwLsSrEFO0e12EAQCZVkiSVI/e7XHhU+XrCq7Qc0FDF2JDLdRzqZ2cu+3Vko3x7Yz3W63C6UUOh2bxsoYGqOXQCkJIYLGYCskSeJdWWVZotfr+3TQNE2ws9NHr9drsmtsgNllUtV17TN/7Gt3/Ew/TdOFPHyXcup2ASulkOcF0jTxfm+7crCbxdzKwbq1OguBd9cXu9vteCNrhUmg0+kgjqPGl2/dVHVd+VTaKApRFLG/R4PBLnZ2dtDv9xHHkc8u6na7/p5bN13auNISMAYEgc1kcn8LzjW4u7uLLEubHdy8ed7uLbGikHrffhhG4Jz5VGAXg4rjGFEU+xTeTqfjU4/j2MZdoihEHJdNCnKnySrr+xRZ52Y8u4lSawNj7P0jiIcIM3dcB9svyWL2ytk6PfOyBvP0zU8+eYm9vT0MBjvI89wHa+taQinlA4tuB3Lbr+3iEDY7x7krjPcTOxeEC166GZ4LxLoxuuO13S/2nKXP8Qds1pDzUc9mM29c3Qw6SRI/Lmd8bbaN8GOa1zFyWTSLLgjGGGazWZMyGfrrnQdW1YIhsq612gfc29lCzs3hRK7tFmqXcnDlNNx9ca4UN8NvZ0DN91nIZp+INfKz2cynbLY/c7cS1Fr5FUs7a+hsbSc3VjcRsH8rGnVtYx7uPrazftzn67Kb2qU72uR5Ds7nwtQOWNe1RJLEC/dBSuk/RzcurTU++eRl8/kzDAYDHBwcLGSiCcF9BhZB3AX3N1oUhV8ZO9x3oV1PbJmsXRScgX7z5gi9Xg/dbsdfcNtf2zaI88wXtmA45mmkAGAWjO/ZeELbsPmLb6Whtt/bLsIGWAPuDG07MOk+LDfW9rjaqabOSLXz692/22OxPnegvUnNve7sx9QO2LaNoHvd2fe3V2jnXXM7RfO81NT2Od17ASyIUHsM7XO3/ybOXvN5n0X7te3ihGe/AO6c7YKE572uHbxuX4t9r/H3+7z70P6bOD4+bgXlM3S73dZ1kCgQy2OTorAR95FL+3SZLu2Lu86X6jZfvJu857LXRlHkfz87I73rDtf2sZfJYrD93Wu76g/rPHFxLHPM541jGX8PZ/eALL738vMDcwFxbjqXMkyxBOIhspFAcxiGTZ2a89tGbvuX7aZjvOz17rn7cN3bPr5V4SYuLnbhHiOIh8haRWFxxnr+l2oZIrEsA3vRcW567KsE4TbH3Bauutf3Qewu4qw77by/TYJ4aGzFXv3Lvly3MSjLMkLnHee2huCindDbyE2u8aIYyXnP35XrjGuZhnqZnz9B3Be2QhRWZRwv+wKfF7i9Drcd67JWLus4xnXHevZYqxa5djD4stdcxV3u47YKOUEsi60QhYu4qxG8bBZ79st9nmvgqvOfzWS56vGL3n/dx68ySMs0+NfhNm69ZZzzrtdwlbuLIB4zWy0KmzRgFxmf81IqLxKY27pWNrkaWSXbPj7gfoyRIFbJVovCNnIdP/Mm4gc0wyUIYhlsJCX1og1MtznOTV0qy/btn7ei2MRsk2a4BEEsg7WuFG46m73q9VdtNjpb0mAbZ9O3DXhvgm0eG0EQy2HtK4V2DZ12LZvL9gOcLT9wWdcrZ7hcTaP2+22RtcuF5Ko4QLtMRLvE87LiANs841/1qm5buE9jJYhlsxFRaJfEdi0jL8M1cq9r2dSdsdVDLyq/UFUVisJ2OXOF2sqyQqeTodfrLYzFvc/1dYjjxK8spLR1911/Ya01JpOxb7bT6/V8NdObbuBa9Qa7beM+jNFxn8ZKEMtmI6Iwr/p5foe1d9/Tfs3572m7jFzF0pOTE4RhBMasKBRFgTwvmuqnBtPpFHGcII5jvH37FkJwPHnyxAvQaGQbvYdh4Av/5XkOwBatcxVDL7vWi1YRq9xgtyruiwARBHF7NioKbfeO+/951TStu2bx/RdV4XRVWWezWdNwZr5ScG0ud3Z2YIzB69evfS+AH/3ohwiCAL1ez4vHy5cvoZTyImIFRyMMQ/R6Pb+COVvd03FRptIqN+stewfx2Uqmyzzesl67TEj4iMfOhjqv2bjA8fEJ3r49RhgGUEphMpmi08kQxzHG4wmklJCyRqfT9b0NqqrCaDTyfYhfvXrVVF2NwbktJ+ua0+zv70NK2zLzxYsXXjCCIITWGoPBoOnMVmFnxzZ5d3X5jdHodrt+NeBaiNrWkEHTUEZ6F5VrlnPdiqNXcRvjtK3G7LIV02Wv3wSXVYQliMfAxlJStbbNbKqqQhSFUEphOp369pNOFGzLSkApCcY4iqLAaDREEASNeIzBGENZxr7hjG3nGKDX6/oexbZ/suvNYIPDcRz5lYfro+zq9xtjfCls1xs4CASyLPMBchdbmPf0XR7LMkh3MW7LioNsu7hdd5VHEI+BtYuC62xVFAXquvazd8YYer0uZrMZxuPxQqetoih8xzBjNIIgwGw2RZ7nvp1kXdfY3d1Fp5MhCAKkaYper4dXr14hz3McHR35Hr/OFWVbZMqm45g9l3NrKaUhBIcxNghuVwkRnj59CgCYzWaYTqeo63qrUzU3uSv8vvBQr4sgbsPaRcHFE6ww2Obptlev7TymtYJtrWmbt7S7sQGAEAGiSPgZvRUU2x7TrSx2dnZ8Q5ROpwPOOSaTadPDWAO4zAgwuJaZC4+yudvL/ps1/ZZtG9C7NtghyGVDENvABkWhRp7btFFnCGazmXf/OP+8EIHvmcw59w3ey7L0PXbjOEKappBSoixLZFnmjbQTBZdRZOMD1tVzVfaTMS7z6V3Xgh2baNxQi32Yb1MobhXpqdfd8X2bc59NCripL36dLptV3N9tXh0SxF1YuyhEUQQpFZIkBecC3W4H3W63cdeIa3+B3ay908nQ7Xawt7ePN2/eoKpqTKdTJEniVw+uETtg4xicW1GoqtK7kuzjdvYvZQ2l7KrDpc4qpSHlvAcz4JrPw7uQXHvRm7Kq9NTr3se7nus2GUrrXBGs4v66CQBBPDQ21qPZuXIYs8a1vVvZrQgYY74Zu0v7DIIAQSAgpQ0WuxWANeQKSimUZeUD0VVVoyznxt2ex56vLEsEgYAQgV+JGDPfseziC27Xshu//Z35VY/b+BYEwZ1npbeZda+abRjHXVYgBEFcnw2lpNogb1VVjUGfQEo7w+/3++h2u36XsE33jLxRd2mhWttSGZ999hmCIMDbt28RBAHCMMR4PIYQNp4wHJ5iOBxhOBwBMF5I3AY3t6u6KAoIIVqb6pjfD5EkCYSwQjaPKQB5njeb2Qy63S6SJLmzQVrmvoDbsK2ZODdZgZAwEMTt2Vh0VCmFXq+Lvb1dH0wuywpxHDcxhcivFOyP3c3c9uVzznFwsN+Uvuig17P7GbKsgySx+xq63S6EEAjD0P/fGN2ISt2cL4aUdqWQpqlPma3rCgBDGAb+fEII1HXdrCyY36Ow7JTUTXETY3pevahtYNvGQxD3iY2JgtYanU4Xu7uDBfcMcL0vtZSyOc4uwjBEmqY4ODjwG9fcMTqdTiMUGaIo8s+3afuHzwsknpevL6UE5xxRZAPjjzH7iIwvQTw81m7JhBCI4wTPnj1DHEc+hnBTXJbSZDJFr9dFv99HGIbvGKqzu40vMmQ3cZmEYYidnYGvxeSyj4iLuU8unfs0VoJYNmsVBfdFE4I3uf3zUtY3/RI6Q9/pdJBl2UJa6NlzuljEbc5zHnaFEMEV57tt1tFD4CbZYveF+zRWglg2G6t95Hz+tyVNUyRJgp2dnYVNZcviMmPHGPOF9s4+/tjYhpIUBEEsj7WKwk0Lo12FE4OrjrnsvQOP1SgtwyDf9RirvPckOASx5nact/3CnbdJyAnB2SA1scgyN1gt4z5v42e1rVlUBLEJ1ioKt+Vs6Qbi+pChuxq6RwQx516IQpvr7mpdBqsWoIcocGevadnX6Habr4NVXwtBbCMbEYVVfbnaboBlnGPVM8h1zFCXcR+uOsZlezquWuXddHyruGcXFUa86bUQxENgI6Jwncqddz0uuQQs64gD3KUI3iYLCJ53vG0s6EcQ62Tt2Uft3y8r/Ha2b/N5X8KLAtDXOf/Zx12RO7v3wP3/6mu57T4LgiCIbWQj+xRc680oihCGIfI893WHnHF1FU/ruvZtNs/Dlbu4TiaS80dXVQ1jNAD4Bj5ZljUltS9/vyumV1WuvwNfSiE8giCIbWDtouBKUud5DsYYgiBYEAWHE4Q8zyFE8I4ozI1zBcZs/+SLhMNhm/LUGI3GTV9lDqW0b9Zj228CcWzFajQawRiDKIqaXswKjMEX74uiCFFkd1Yv8/5cx722LSK0TWMhCOLubKTzWl3XGA6HTR2kGCcnpwiCALu7u97AWEEocHo6RBTFTcvOOUVRoCxL5HnR9GROIIS4tAZRXdeYzWb45JNPUBQFkiTx/RhsNVUbYtnf30Ov18P3v/99KKWws7OD8XiCPM/R63UBAGVZodfrodPJ0O/3l3Z/7lvJiGVuZiOBIYjNs7HSnkFg22yenp5iNptCCIHj42OEYQjOOY6PT1BVFcqyRFHkTbE7Z9hznJ6eoqpKdLs9X2L79PQUxhjfmrMoSl90zzXM0Vr7vgvGaGRZiiiKUZYFOOfY3R34Xg+2phEHwBBFUSu2wXxfZvv8IncxbjdtqbkthvS247iPyQHbcs8JYhVsTBQYY97o21aYHOPxGFFkC9uNRiNIKWGMQVGUCMMCjNm2mcPhqBGFClEUg3OOug4xmUyglPTundlshixLffMc1w/BCoLtpxCGIbIsRVEUAKyguI5qwLwTnBMrY4yPJZxXgM9d213uy9l/XxSIX2bJkKu4yhDeRMjuI+1remjXRhBtNiIKLmA7Ho8xnc6auACDlMoHi8fjsW+/CQDT6bTx60uUZYnZLIeUEsfHxxBCgHOOPM+hlEIQBJDS9lhO0xRhGKLX6/kVwpMnT2CMwaeffgqAoa4lZrMZhBDI89w3zXGPvffecxRFgbqW6HQ6viXobDZrxGO59+Y6ZbzXbZguEqXrvPYh8BCviSDOY+2ioJTyBtt2NQt9C8yyLBfcM7YDmvYiUlUVOLdtNhnjkFIiDEMfpwjDEEFg+y0ztngMF79w1VWt6yhDXdeYTMao6xpa62Z1YUtx28wm44+htUIcRwiCEEJwL1Tz3s83Nxxn33fXGbd7z6pn6zcRiYfGY7pW4vGxdlGo6xpVVSHPCz+Dt2miFfI8hzHW4Lz//nsAgMlk4t02UkrfYS3Pc2+Qi6JAVVUYDAYIggBHR0dNN7YE0+kULibQ6/V8pzelFHZ3d/HZZ5/hzZs3UMrGGk5PT5s+0JF3X0lpU1BdeqztpYAmi6rwwgVcb2Pedf3od/HRb3ol8ZB5TNdKPD7WLgq2E1q04KMHgCAQ6PW6iCJrdIXgkNK6ZtxM/KK6N+3slbmv3T4+GOyCMSsucRxDa+3jBGmaIssyZFkHjMHHDpTSKMsS+/v70FpjNBoBsI16giDwohKGEdI0uVGl1vvue1/XmC86z328ZwRxn1i7KLSDtC4byBp6hiAQSJIEcWyzgdoz9fYO56qqfDaREwJnqAEnCjZrKAyDxjU1QVWVTW9lAcbmu5JdTIJzBmPgXVVhGDZZTEWzp0IsiJMQ3Hdd24ZyDetgXWO+bbyCRIMg7sZGAs2ccySJ3Z/gNogxxhrjbMXi008/BWMM3W4XJyfHqOsa+/v7yPMZ3r49RpIkCALRHI8hjiMcHx9DKYX9/X0EQQAhBE5ObGqr24HMGMfz589gjMH3v/99zGZTzGa5D2hrrZDnNm5gs5lUE2uYQimFw8NDdLvdZqOcEx/CcZlRPm9PwrL3KSxz38RNniOIh8LaRWFuAIBOp4M4jhfcPS7/XykFxhg6nQydTgdSSgwGO1BKI8ty35PZGA3OBcIwQBTF0FphMBiAc9G4gwK/+zlNM6Rp6t1WaZqAMdfvOQHnzO9jcHEFrTUYYygKm+0URXbfg9YGxsxjCY+NiwzkdWMk58U+tsHgLjvGQxD3jY3tUzDGYDAYoNvtNDWHAGO0d8VYw859iQmlFJIkgda6tbGM+RhBGIYoyxLGGCTJfPdzVfUX3EQ2nTSA1ho7OztecHq9nt8N7cZQliUAu9GuqiporZvMJ+ZdWq6G0qru0SYN0bLSTjd9HbfhPo6ZIJbBRmIKYRii08mQZTY99GxpCrdicMbZbTbj3Lpq2ruLncF3G9Pc7472sd17XPG8Xq/n4xpuBdA2Bq6WEmOsSWM1XlAAhThOmp3WqzEey0hxXff5V3mcdXIfx0wQy2Aj7iMbU0gQRZHP5jn7JbyshtFFz7XF4KLHzhr9edBYvOPjvug87jX2/efval41t3HfXPa+ZbHpGfYqCwpu+toIYh1sxH0UhiH29vY2sgHqrA/7OruHz+IC5cDyqqPelNvep3XeX2D9hvQ657rueG6yp4QgHgprFYWrvmC3/bLedSzLeu+2ziRXNa7rHHcb70ebx1augyCu4t7kU7azfG5T+mEV4zjLZeNaZZbSVce+aVD4umyr0XwI10AQm+LeiMIqZvbrPNYqjc82XN+q2VZDP99Fv7ZTEsRKWasoXPTF3uQselu4bJzrvoZlf07LGP+6xeqmY77trnaC2DbWKgqb8N3ely/qNm2aWvbndF8+gzbXHbMrtXIfr5EgzmMtgeZ5vaB1nI0g1sHiHzOtFIiHwppEAdDaNJu+COIhMN8lT2JAPCRWLgqu0J3dtEZfHuJh4SrskjAQD4W1rBSsGHByHxEPCrc7/7zifgRxX1mTKHBwDhhzbzJgCeJakBAQD421uI+A7d3tSxB3hf6uiYfE2spc0BeHIAhi+yF/DkEQBOHZWJMdgiAI4nKMMb7nC7AejwuJAkEQxJYipfSdJ90GyXZnyVWwclFwNWQWN69dLzfV3gQOzmm3KEEQjwtjDKbTKYqiwGw2QxAECMMQaZpe2oTsrqyxzIX2dWKui80DBxjjAFwu+IoGSRAEsWUURYHpdIrxeIwwDBHH8Y3t6E1Ziyi4FcLNdn66Wkn2/yQGBEE8NlxMQUoJzjmUUiuvmrzWmILd2Xx1/1zANK4j63Zqnmn+T+pAEMTjYd2l89e6T4HzqzNglbJupiCwQ7Ori1WPjiAIggDWWCXVGfbJZAKtDYQQCMMQYRhAKQWlFIqixGg0Rp7n2N/fRxxHSNN5pJ2CzQRBEKtlbSsFWz5bI88LSKkQBAHS1EAI6yerqhqz2QzD4QjT6QxJkgLAgigQBEEQq2VtoiBlDSklJhObYpXnOQ4PD8DYHqSskecF3rw5Qr/fx9Onh4141OsaHkEQBIE1lrlw+xSiKEIURbABY+afU0qjqmoIIZAkCYSwaairTr8iCIIg5qzRfWSgtUG/30OSJKiqGmEYgHPm067q2qZdxXGEMIyaFCzd7FMgCIIgVs3aRIFzjiAImt4KHAcHe82KQCCOYwgh8NFHH6Df7zXdrFgr64hWCgRBEOtgI7WPGGNI0wRBEIAx5sXCCkREWUYEQRAbYm2iMJvlGI3GEIL7PQudToYsS8EYg9YGs1kOzu1zdgef8EWgCIIgiNWzxs1r9mc2y8EYkCRJ042NwxgNwPi9ClLafQvO1UQQBEGshzXGFOxmtaOjYxhjEIZh8ziDMXYloJTCeDzx2UbWrUQrBYIgiHWxNlFIkgRRZHcwG2OQZSmiKG5WAgxRFKLf72E8niDPC+zuDhDH0bqGRxAEQWCNohAEApzPU1DDMPSuIVsoTyBJYhRFibqWTVpquK7hEQRBEFh7TIEhjuNzn+ecI4oiHB7uwxgDzrnf29De6EYQBEGsjjWJgoHWaALK1+uN4HYya21u2IeBIAiCuC1rWyncpVzF4kY2giAIYlWsRRRsmQoFYzRuoguMMb+vgSAIglg9axIF1tqEdpMezWffSxAE8XhwlR6CIIAQAkKIldvCtYiCdf8IGHOzjWju4kkQCIJ4jKRp6pNw2uKwSpu4clG4y+BJDAiCeKy4bE1XTNSVB1p1lYe19mgmI08QBHF94jhGEAQ+ld8Jw71eKRAEQRC3Y95rpl5bLTiqNkcQBLGlGGMLheZ5jqqq1tKJkkSBIAhiS1FKYTab4Qc/+AHevn2LsixJFAiCIB4rnHOEYYh+v480TRGG4cNISSUIgiBujhOFnZ0dpGnqs5BWCYkCQRDEluLaFe/s7KytBhyJAkEQxBbjhGFdUEyBIAjiXrHl7iPGbNqUMWh6LrOVR8cJgiAeMtaEmndsKzCvCbcqlrImYYxBqev3SiAIgiAuxwlCu4joOipDLM1RxRigtd19RxAEQdwdIdwuZgPGODhn4Fw0LYxXw9JWCrYmB0CaQBAEsRyEEACsO2mxjcDqRIGZOwYAKH5AEASxbubxhWWzhEAzBREIgiAeCpSSShAEQXhIFAiCIAgPiQJBEAThIVEgCIIgPCQKBEEQhIdEgSAIgvCQKBAEQRAeEgWCIAjCQ6JAEARBeEgUCIIgCM+dy1ycrX1EpZAIgiCWz3kVhVZRZmhppbPrWkJrTQXyCIIgVoATgDAMt790tu0KhGag5JEiCIJYHcaX0l4FSxIFgHNb95vz1XcGIgiCeGxYT8zqz3Pnab0bpO0betejEQRBEOdhG+zY31cpDkv19dAKgSAI4n5DAQCCIAjCQ6JAEARBeEgUCIIgCA+JAkEQBOFZ2ua1i2hvZjPG5dfOg9IUnCYI4jHibKO1i+Ydm7gp27hyUXDUtYSUdtczYxxRFEIITqJAEMSjRSkNKSXqugbnHJxzxHG0Ubu4lpVCXduLrqoaWhtwzmCMQRSFCENaMRAE8XhwKwStNcqybERBgjEGIQQ4ZxAiQBCIjYxvZaLQXhpVVY2yrFCWJQArAHbFAAhh1ZEgCOKxYIyB1hpFYUVBSgUAEEJACI4oYhDC2sV1T5hXulLQWqOuJfK8gFLKP+5WD0FQNzdgs8slgiCIdbLoPdH+cScUgBUFIda/WljpFF1rDa01lFLvVE+1j2tIqWDMPNhCEATxUHE2TikFpdSCILjnldL+uU3YxJWKglPDiy5OSomyrGCMPufdBEEQDw/rKbGrhPNQSkFKiaqSGxGFlbiP5mpoVwPtx4bDIbRWECJAp5MhyzJorcE5ZSIRBPGwsbEEuxqoa4nRaAjGrJsojmPvLtLaNF6U9XtQVhpo1lotxBK01hgOT1FVFeI4BucMcRx7N9Mm/GcEQRDrwpi5W11KidPTUwgRIIoiBEHgk24ucruvg5WtFOa+MbtSKMsSs9kMH3/855jNZgjDAF/4wk8gyzqQUoFzAdIEgiAeNsa708uywJ/8yZ+gKApIKfGX/tJfwv7+AYIgaOyngtaLG9vWwcpiCk4YnNLVdY3ZbNao3zwAbTe0UZCZIIiHT9suWsMvURQFRqMRRqMRptNJ63UawINZKVifWNvYT6dTHB29Qa/XRa/XRRiGiKIQRVF4RSQIgnjIGAPvFuJcYH//EMYAs9kMP/zhDzGdzvCzP/uzYIz5+MO6XesriikY/+Mi7UWRYzQaIc9zMMbQ7/ebzKTNBFMIgiDWTdvWMcaQJDF2d3cRRREmkwmUUhgOh+h0MqRp6uvFrZOVBprd/gMpJYrCxhTG4zGE4IjjuOU62swyiSAIYt1Ym2dFwW3cTZIUb968xmw2xenpKTjnSNN0I+NbmfvIqZuUEm/evMF4PIKUEmmagHMOKSUA+GURrRQIgnj4zO0cYwxhGOLly5f4zne+g6OjNwCA2SzHV77yFQwGP7cRL8qKiw7ZCPp0OkFVVc1jrPkBOOcIgsD/myAI4qHTNvIuq8ht4NXaoKoqP2neBCutfWSMXSkcHx9jNpst1A3nnCMMIyRJ0uTmkjAQBPG40NogTVM8efIUUtp9XYPBTuM6chWk1zumlYiCa6Lj8nHDMESapgiCANPpFJxz9Ho9JElMO5kJgnhEMHDuVgc2CUeIADs7Oz6W8OUvfxlPnz4DY2ztggCsfKVgRcG6iGz8IM9zCCHQ6XQQRZHfwUe6QBDEY2DuMjLeTRRFEdI0RRRF+PDDj5BlWfM69jBKZ1v3EPP/F0IsVAQMggAHBwfIsgwAfMchgiCIh4zzojBmS1mMRkN89tlnePnyJX72Z38WT58+RbfbQxAEzevW71pf2UrBxQ0AhrIsUZYlqqoC57Z/QpIkECLwr6WVAkEQDx1nF93mNLd5NwxDDAYD7O7uQgjR2ETWiMh6x7iylQJjzKvdcDhEnucoyxL7+/vodrvIso6/eHuTaKVAEMTD5qwoTCYTBEGAZ8+e47333sfBwYF/XftnnaxspcA5b2IHGb70pZ9BVVWo6xpxPM84mruX5sEXgiCIh8t8EpwkCb785S8DAIQI0Ov1vADYfs18I4k4K1spGGP8zuVnz2y6lQ2qGL9hjTH4G0QZSARBPHSczePcblx7/vw9P4F2nhUA4Hy+omgHptfBSrOPXAnYIAgRBOE7FyWEQBgGXhEJgiAeMtY7AgSBQBSFPtnmLEKIjfWuX6koCMGhtYAQAlrrdy7QKmRAqwSCIB4N1jUkIMS7NhGYZ2PamOv6x7diURAwBq0Wc/NezDYQbdVyE8EUgiCIdeNc62FoTW+eF2fKXtjJdBgGiKLNTJhXKgpW7YA0jVFVtlG1K3MRRSHCMFzwoxEEQTwG7ETZls6WUqKupQ8uJ0mCMNycXVzpPgUAPqBim1UrGDN/LAhsPIEgCOIx4WKobsWgtfFupTAMwLnYmCgwc8eQ9ryHKBbSqRzu8LZfs/IrBdukev46Wi0QBPEYaJtc14vZ9qnnTTnt+abes+9rN91xVSOWzUrdR8DiigEQsPXE5/sSSAwIgnhMtFNMbTaSQBAsblTbpF1cuSg4bER9XWcjCILYbpzhF4JtlRt9e0ZCEARBbBwSBYIgCMJDokAQBEF4SBQIgiAID4kCQRAE4bmzKLQzp9ZVxY8gCOIxs8qM1aWkpDL27oYMgiAIYnm4jWtWEFanCkvbp2B7MBsSBIIgiBXhdjyvcnPbklYKrKnVQaJAEASxCtwKYdW7ne9c+6gNCQJBEMTqWEf5C8o+IgiCIDxLrX1Exe0IgiDuN7RSIAiCIDwkCgRBEISHRIEgCILwkCgQBEEQHhIFgiAIwkOiQBAEQXhIFAiCIAgPiQJBEAThIVEgCIIgPCQKBEEQhIdEgSAIgvCQKBAEQRAeEgWCIAjCQ6JAEARBeJZaOnvdnO0LzdjquxI1JwMA6Ob/qzrveU2Lzp7HvUYp1XTA47DdmS47bvt4lz9/2euuizuegQGaHrPn3a+FHrTN81SMnSDWy4NYKWitobRee+c3rTWUUis/h/u5CGMMyrJCVdXnGvTbnbf5WeIt1RpQF18GjLHPL/OcBEHcjDuvFLRW0Fohz0sopcGFQBSFiKMIumWoGecQnPv3lGUJrTS0MRBBACEEojC88Yxb1hWKosJ4OsNOv4c4jsA4t7PmW05vx8MhppMxJASiOMbh/u7CuIwxMEphOBqjqCo8fXKIQAgAQD6bQUoJxgW4EAhEgCDgzQx+EWMMYAyKokSe51CyglQas6JGliXI0hRGK4AxcBEiiSNEUXjm+mvUVYkf/vhTJGmKDz94AQaD8+bYxhhoDQynGq9ONJ7sCnQShjhcnLlrbTCaGoABjAPdhEFwQCmglgaVBDgDhADS+OJVkjEGs9xgWhgcjTWSiCEOGZ7ucQg2H6ETsllpcDo22N3hiENAsHMvgyCIFXJnUTDGQCuFqqxQSwkRBOCcI4qwIAptA220gaxrSKmgtEZggBBAFIYXnOXCs0MriaqqMJ3myNIUYRiAM2ZtyS1FoSwLjMcjVAiRKY3D/d13zmuMRl4UmOYFnmgDWE1ALWtUZQkmQgRBAM44jLlsQWZQ1zVmsxyyylHXEqeTEkr17D0zCmAcIgDC4N2Py2gFWdc4GQ7RVcoKlrv+885mgLw0eHOq0etwRBFDfOY12gBlZQAOcMHQaBeMsaJQ1lYUQmNF4cIrM0BVG0xzbc+XMXRSjicXrARqCUxzg17HIApJDQhiEzCzbp8LQRAEsbU8iJgCQRAEsRxIFAiCIAgPiQJBEAThIVEgCIIgPCQKBEEQhIdEgSAIgvCQKBAEQRAeEgWCIAjCQ6JAEARBeP5/zc1EET8bNooAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize a image in order to know if data is loaded properly or not\n",
    "import numpy as np\n",
    "\n",
    "# Get a batch of images\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "print(image.shape, label)\n",
    "\n",
    "# Plot image with matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53e072",
   "metadata": {},
   "source": [
    "# Step 1 \n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "1- turn an image into patches\n",
    "\n",
    "2- flatten the patch feature maps into a single dimension\n",
    "\n",
    "3- Convert the output into Desried output (flattened 2D patches): (196, 768) -> N×(P2⋅C)       #Current shape: (1, 768, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d44424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class which subclasses nn.Module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    \"\"\" \n",
    "    # 2. Initialize the class with appropriate variables\n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
    "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
    "                                  end_dim=3)\n",
    "\n",
    "    # 5. Define the forward method \n",
    "    def forward(self, x):\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        \n",
    "        # 6. Make sure the output shape has the right order \n",
    "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f238d6",
   "metadata": {},
   "source": [
    "# PatchEmbedding layer ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d991f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([1, 3, 224, 224])\n",
      "Output patch embedding shape: torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# Let's test it on single image\n",
    "patch_size =16\n",
    "\n",
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "set_seeds()\n",
    "\n",
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3bae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0113,  0.2451, -0.2154,  ...,  0.7446, -0.4476,  0.3765],\n",
      "         [-1.0104,  0.2385, -0.1778,  ...,  0.7155, -0.4246,  0.3198],\n",
      "         [-1.0417,  0.2837, -0.2902,  ...,  0.7866, -0.4685,  0.3662],\n",
      "         ...,\n",
      "         [-1.0430,  0.2815, -0.2894,  ...,  0.7875, -0.4666,  0.3651],\n",
      "         [-1.0430,  0.2815, -0.2894,  ...,  0.7875, -0.4666,  0.3651],\n",
      "         [-1.0426,  0.2868, -0.2914,  ...,  0.7946, -0.4700,  0.3664]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "Patch embedding shape: torch.Size([1, 196, 768]) -> [batch_size, number_of_patches, embedding_dimension]\n"
     ]
    }
   ],
   "source": [
    "# View the patch embedding and patch embedding shape\n",
    "\n",
    "\n",
    "print(patch_embedded_image) \n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111bb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([3, 224, 224])\n",
      "Input image with batch dimension shape: torch.Size([1, 3, 224, 224])\n",
      "Patching embedding shape: torch.Size([1, 196, 768])\n",
      "Class token embedding shape: torch.Size([1, 1, 768])\n",
      "Patch embedding with class token shape: torch.Size([1, 197, 768])\n",
      "Patch and position embedding shape: torch.Size([1, 197, 768])\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0113,  0.2451, -0.2154,  ...,  0.7446, -0.4476,  0.3765],\n",
      "         [-1.0104,  0.2385, -0.1778,  ...,  0.7155, -0.4246,  0.3198],\n",
      "         ...,\n",
      "         [-1.0430,  0.2815, -0.2894,  ...,  0.7875, -0.4666,  0.3651],\n",
      "         [-1.0430,  0.2815, -0.2894,  ...,  0.7875, -0.4666,  0.3651],\n",
      "         [-1.0426,  0.2868, -0.2914,  ...,  0.7946, -0.4700,  0.3664]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now add the the learnable class embedding and position embeddings\n",
    "# From start to positional encoding: All in 1 cell\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# 1. Set patch size\n",
    "patch_size = 16\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True) # make sure it's learnable\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n",
    "#patch_and_position_embedding\n",
    "\n",
    "print(patch_embedding_class_token)  #1 is added in the beginning of each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecaae3",
   "metadata": {},
   "source": [
    "Here we're only creating the class token embedding as torch.ones() for demonstration purposes, in reality, you'd likely create the class token embedding with torch.randn() (since machine learning is all about harnessing the power of controlled randomness, you generally start with a random number and improve it over time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb4eca",
   "metadata": {},
   "source": [
    "# Lets create layers used in Transformer's encoder:\n",
    "\n",
    "\n",
    "#### Norm (LN or LayerNorm) - torch.nn.LayerNorm().\n",
    "\n",
    "Layer Normalization (torch.nn.LayerNorm() or Norm or LayerNorm or LN) normalizes an input over the last dimension.\n",
    "\n",
    "Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).\n",
    "\n",
    "\n",
    "#### We can implement the MSA layer in PyTorch with torch.nn.MultiheadAttention() with the parameters:\n",
    "\n",
    "Multi-Head Self Attention (MSA) - <b>torch.nn.MultiheadAttention()</b>\n",
    "\n",
    "    embed_dim - the embedding dimension D .\n",
    "\n",
    "    num_heads - how many attention heads to use (this is where the term \"multihead\" comes from)\n",
    "\n",
    "    dropout - whether or not to apply dropout to the attention layer \n",
    "\n",
    "    batch_first - does our batch dimension come first? (yes it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5effd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95822a",
   "metadata": {},
   "source": [
    "# MLP Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61bd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "    \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4066b0e",
   "metadata": {},
   "source": [
    "# Creating a Transformer Encoder by combining our custom made layers\n",
    "\n",
    "In below cell we are creating transformer encoder ourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c1ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    # 5. Create a forward() method  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408ef42",
   "metadata": {},
   "source": [
    "#### Transformer Encoder block created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114d4c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "TransformerEncoderBlock (TransformerEncoderBlock)  [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "├─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─LayerNorm (layer_norm)                      [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "│    └─MultiheadAttention (multihead_attn)         --                   [1, 197, 768]        2,362,368            True\n",
       "├─MLPBlock (mlp_block)                             [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─LayerNorm (layer_norm)                      [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "│    └─Sequential (mlp)                            [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─Linear (0)                             [1, 197, 768]        [1, 197, 3072]       2,362,368            True\n",
       "│    │    └─GELU (1)                               [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
       "│    │    └─Dropout (2)                            [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
       "│    │    └─Linear (3)                             [1, 197, 3072]       [1, 197, 768]        2,360,064            True\n",
       "│    │    └─Dropout (4)                            [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "==================================================================================================================================\n",
       "Total params: 7,087,872\n",
       "Trainable params: 7,087,872\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.73\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.61\n",
       "Forward/backward pass size (MB): 8.47\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 27.98\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "from torchinfo import summary\n",
    "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "       row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d076ace",
   "metadata": {},
   "source": [
    "# Let's build a vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b02def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        \n",
    "        # 3. Make the image size is divisble by the patch size \n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        \n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1) \n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32280991",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9288fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our MOdel\n",
    "\n",
    "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = ViT(num_classes=len(class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b421e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\vision_transformer\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:32<00:00, 33.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.8393 | train_acc: 0.5000 | test_loss: 9.8899 | test_acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \n",
    "optimizer = torch.optim.Adam(params=vit.parameters(), \n",
    "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
    "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
    "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
    "\n",
    "# Setup the loss function for multi-class classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the seeds\n",
    "set_seeds()\n",
    "\n",
    "# Train the model and save the training results to a dictionary\n",
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=1,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1a0a8",
   "metadata": {},
   "source": [
    "Our ViT model has come to life!\n",
    "\n",
    "Results on our custom dataset don't look too good.\n",
    "\n",
    "Lets plot the accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd44080e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To check out our ViT model's loss curves, we can use the plot_loss_curves function from helper_functions.py\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_loss_curves\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot our ViT model's loss curves\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plot_loss_curves(results)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helper_functions'"
     ]
    }
   ],
   "source": [
    "# To check out our ViT model's loss curves, we can use the plot_loss_curves function from helper_functions.py\n",
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# Plot our ViT model's loss curves\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f575c15",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6fd91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"C:\\\\projects\\\\vision_transformer\\\\dataset\\\\train\\\\Bad\\\\Screenshot 2024-11-20 150637.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180aa54",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(custom_image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Predict on custom image\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mpred_and_plot_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\projects\\vision_transformer\\going_modular\\going_modular\\predictions.py:40\u001b[0m, in \u001b[0;36mpred_and_plot_image\u001b[1;34m(model, class_names, image_path, image_size, transform, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predicts on a target image with a target model.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    device (torch.device, optional): Target device to perform prediction on. Defaults to device.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Open image\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Create transformation for image (if one doesn't exist)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\projects\\vision_transformer\\venv\\lib\\site-packages\\PIL\\Image.py:3480\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m   3478\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3480\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m   3482\u001b[0m preinit()\n\u001b[0;32m   3484\u001b[0m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Import function to make predictions on images and plot them \n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = \"C:\\\\projects\\\\vision_transformer\\\\dataset\\\\train\\\\Bad\\\\Screenshot 2024-11-20 150637.png\"\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c87c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def pred_and_plot_image(model, image, class_names, image_size=224, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Predicts and plots results for a given image using a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        image: PIL image object.\n",
    "        class_names: List of class names.\n",
    "        image_size: Target size for the image (default: 224).\n",
    "        device: Device to run the model on (e.g., \"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.inference_mode():\n",
    "        # Ensure image is in RGB format\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        # Preprocess image\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        transformed_image = image_transform(image).unsqueeze(dim=0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        target_image_pred = model(transformed_image)\n",
    "        pred_class = torch.argmax(target_image_pred, dim=1).item()\n",
    "\n",
    "        # Output the result\n",
    "        print(f\"Prediction: {class_names[pred_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "504e3d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Good\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image directly\n",
    "custom_image = Image.open(\"C:\\\\projects\\\\vision_transformer\\\\dataset\\\\train\\\\Bad\\\\Screenshot 2024-11-20 150637.png\")\n",
    "\n",
    "# Predict on the loaded image\n",
    "pred_and_plot_image(model=vit, image=custom_image, class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a1a53",
   "metadata": {},
   "source": [
    "# Next part - Create Image classifier using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb9ce187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(custom_image_path).convert(\"RGB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
